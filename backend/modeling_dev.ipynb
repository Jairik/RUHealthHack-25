{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f8fc6e",
   "metadata": {},
   "source": [
    "## Dev file by Logan Kelsch\n",
    "### This file is used for initial instantiation of model ideas, initial runs and tests, and temp code storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5cbb5f",
   "metadata": {},
   "source": [
    "### Depricated Train and test for Neural-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68737274",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msspec_nn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jairi/OneDrive/Desktop/Repos/RUHealthHack-25/backend/sspec_nn.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#imports\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import sspec_nn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"temp_data/train.csv\")\n",
    "\n",
    "X = data.values[:, :-1]\n",
    "y = data.values[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "model = sspec_nn.sspec_nn_train(X_train, y_train, batch_size=16, epochs=250, optimizer='adam')\n",
    "y_proba, y_pred = sspec_nn.sspec_nn_predict(model, X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.imshow(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393043b3",
   "metadata": {},
   "source": [
    "### Softmax (medical) condition classifier training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ebe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update function to do per-class splits guaranteeing at least 1 sample in test and 1 in val for every class.\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, top_k_accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SAVE_DIR = Path(\"./symptoms/model/\")\n",
    "\n",
    "def train_softmax_classifier_classwise(\n",
    "    data_path: str,\n",
    "    save_dir: str,\n",
    "    random_state: int=42,\n",
    "    epochs: int=6,\n",
    "    batch_size: int=128,\n",
    "    lr_alpha: float=1e-5,\n",
    "    learning_rate: str=\"optimal\",\n",
    "    eta0: float=0.0,\n",
    "    ngram_range_word=(1,2),\n",
    "    ngram_range_char=(3,5),\n",
    "    max_features_word: int=40000,\n",
    "    max_features_char: int=50000,\n",
    "    min_df: int=2,\n",
    "    early_stopping: bool=True,\n",
    "    patience: int=2,\n",
    "    verbose: bool=True\n",
    "):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    data_path = Path(data_path); save_dir = Path(save_dir); save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df = pd.read_csv(data_path)\n",
    "    texts_all = df[\"user_input\"].astype(str).to_numpy()\n",
    "    y_all = df[\"target_condition_id\"].astype(int).to_numpy()\n",
    "\n",
    "    # keep classes with >=3\n",
    "    uniq, counts = np.unique(y_all, return_counts=True)\n",
    "    keep = uniq[counts >= 3]\n",
    "    mask = np.isin(y_all, keep)\n",
    "    texts = texts_all[mask]; y = y_all[mask]\n",
    "\n",
    "    # map to indices\n",
    "    unique_ids = np.array(sorted(np.unique(y).tolist()), dtype=int)\n",
    "    id_to_idx = {cid: i for i, cid in enumerate(unique_ids)}\n",
    "    y_idx = np.array([id_to_idx[int(v)] for v in y], dtype=int)\n",
    "\n",
    "    # build per-class indices and split: 1 to test, 1 to val, rest to train\n",
    "    by_class = {c: np.where(y_idx == c)[0] for c in range(len(unique_ids))}\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for c, idxs in by_class.items():\n",
    "        idxs = idxs.copy()\n",
    "        rng.shuffle(idxs)\n",
    "        if len(idxs) >= 3:\n",
    "            test_idx.append(idxs[0])\n",
    "            val_idx.append(idxs[1])\n",
    "            train_idx.extend(idxs[2:])\n",
    "        else:\n",
    "            # shouldn't happen (we filtered), but fallback: 1 test, rest train\n",
    "            test_idx.append(idxs[0])\n",
    "            train_idx.extend(idxs[1:])\n",
    "\n",
    "    X_train_text = texts[train_idx].tolist()\n",
    "    X_val_text = texts[val_idx].tolist()\n",
    "    X_test_text = texts[test_idx].tolist()\n",
    "    y_train_idx = y_idx[train_idx]\n",
    "    y_val_idx = y_idx[val_idx]\n",
    "    y_test_idx = y_idx[test_idx]\n",
    "\n",
    "    # vectorizers\n",
    "    v_word = TfidfVectorizer(analyzer=\"word\", ngram_range=ngram_range_word, min_df=min_df, max_features=max_features_word, lowercase=True)\n",
    "    v_char = TfidfVectorizer(analyzer=\"char\", ngram_range=ngram_range_char, min_df=min_df, max_features=max_features_char, lowercase=True)\n",
    "    Xw_tr = v_word.fit_transform(X_train_text); Xc_tr = v_char.fit_transform(X_train_text); X_tr = hstack([Xw_tr, Xc_tr], format=\"csr\")\n",
    "    Xw_va = v_word.transform(X_val_text); Xc_va = v_char.transform(X_val_text); X_va = hstack([Xw_va, Xc_va], format=\"csr\")\n",
    "    Xw_te = v_word.transform(X_test_text); Xc_te = v_char.transform(X_test_text); X_te = hstack([Xw_te, Xc_te], format=\"csr\")\n",
    "\n",
    "    # class weights\n",
    "    classes, counts_tr = np.unique(y_train_idx, return_counts=True)\n",
    "    freq = counts_tr / counts_tr.sum()\n",
    "    w = 1.0 / np.maximum(freq, 1e-12)\n",
    "    weight_map = {int(c): w[i]*(len(w)/w.sum()) for i,c in enumerate(classes)}\n",
    "\n",
    "    clf = SGDClassifier(loss=\"log_loss\", alpha=lr_alpha, learning_rate=learning_rate, eta0=eta0, random_state=random_state)\n",
    "    classes_all = np.arange(len(unique_ids), dtype=int)\n",
    "    n_train = X_tr.shape[0]\n",
    "    best_val_f1 = -1.0; best_epoch = -1\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        order = np.arange(n_train); rng.shuffle(order)\n",
    "        for start in range(0, n_train, batch_size):\n",
    "            end = min(start + batch_size, n_train)\n",
    "            idx = order[start:end]\n",
    "            xb = X_tr[idx]; yb = y_train_idx[idx]\n",
    "            sw = np.array([weight_map[int(c)] for c in yb], dtype=float)\n",
    "            if epoch == 1 and start == 0:\n",
    "                clf.partial_fit(xb, yb, classes=classes_all, sample_weight=sw)\n",
    "            else:\n",
    "                clf.partial_fit(xb, yb, sample_weight=sw)\n",
    "        # validation\n",
    "        proba_va = clf.predict_proba(X_va); pred_va = np.argmax(proba_va, axis=1)\n",
    "        acc = accuracy_score(y_val_idx, pred_va)\n",
    "        macro_f1 = f1_score(y_val_idx, pred_va, average=\"macro\")\n",
    "        top3 = top_k_accuracy_score(y_val_idx, proba_va, k=min(3, proba_va.shape[1]))\n",
    "        top5 = top_k_accuracy_score(y_val_idx, proba_va, k=min(5, proba_va.shape[1]))\n",
    "        if verbose:\n",
    "            print(f\"[Epoch {epoch}] val_acc={acc:.3f} macroF1={macro_f1:.3f} top3={top3:.3f} top5={top5:.3f}\")\n",
    "        if macro_f1 > best_val_f1:\n",
    "            best_val_f1 = macro_f1; best_epoch = epoch\n",
    "            dump(clf, SAVE_DIR / \"sgd_softmax_best.joblib\")\n",
    "            dump(v_word, SAVE_DIR / \"tfidf_word.joblib\")\n",
    "            dump(v_char, SAVE_DIR / \"tfidf_char.joblib\")\n",
    "            with open(SAVE_DIR / \"label_map.json\", \"w\") as f:\n",
    "                json.dump([int(x) for x in unique_ids], f)\n",
    "        if early_stopping and (epoch - best_epoch) >= patience:\n",
    "            if verbose:\n",
    "                print(f\"[EarlyStopping] no improvement for {patience} epochs (best epoch {best_epoch}). Stop.\")\n",
    "            break\n",
    "\n",
    "    # test eval\n",
    "    clf = load(SAVE_DIR / \"sgd_softmax_best.joblib\")\n",
    "    proba_te = clf.predict_proba(X_te); pred_te = np.argmax(proba_te, axis=1)\n",
    "    acc_te = accuracy_score(y_test_idx, pred_te)\n",
    "    macro_f1_te = f1_score(y_test_idx, pred_te, average=\"macro\")\n",
    "    micro_f1_te = f1_score(y_test_idx, pred_te, average=\"micro\")\n",
    "    top3_te = top_k_accuracy_score(y_test_idx, proba_te, k=min(3, proba_te.shape[1]))\n",
    "    top5_te = top_k_accuracy_score(y_test_idx, proba_te, k=min(5, proba_te.shape[1]))\n",
    "\n",
    "    report = {\n",
    "        \"classes_kept_ge3\": int(len(unique_ids)),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"test_accuracy\": float(acc_te),\n",
    "        \"test_macro_f1\": float(macro_f1_te),\n",
    "        \"test_micro_f1\": float(micro_f1_te),\n",
    "        \"test_top3_accuracy\": float(top3_te),\n",
    "        \"test_top5_accuracy\": float(top5_te),\n",
    "    }\n",
    "    with open(SAVE_DIR / \"report.json\", \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    # inference script\n",
    "    infer_code = f\"\"\"#!/usr/bin/env python3\n",
    "import json, sys\n",
    "from joblib import load\n",
    "from scipy.sparse import hstack\n",
    "word = load(r\"{(SAVE_DIR / 'tfidf_word.joblib').as_posix()}\")\n",
    "char = load(r\"{(SAVE_DIR / 'tfidf_char.joblib').as_posix()}\")\n",
    "clf = load(r\"{(SAVE_DIR / 'sgd_softmax_best.joblib').as_posix()}\")\n",
    "with open(r\"{(SAVE_DIR / 'label_map.json').as_posix()}\") as f:\n",
    "    label_map = json.load(f)\n",
    "def predict_topk(text, k=5):\n",
    "    X = hstack([word.transform([text]), char.transform([text])], format='csr')\n",
    "    p = clf.predict_proba(X)[0]\n",
    "    idxs = p.argsort()[::-1][:k]\n",
    "    return [(int(label_map[i]), float(p[i])) for i in idxs]\n",
    "if __name__ == \"__main__\":\n",
    "    txt = \" \".join(sys.argv[1:]) or \"pelvic pressure and heavy flow\"\n",
    "    print(predict_topk(txt, k=5))\n",
    "\"\"\"\n",
    "    (SAVE_DIR / \"inference.py\").write_text(infer_code)\n",
    "\n",
    "    return {\"report\": report, \"paths\": {\n",
    "        \"report\": str((SAVE_DIR / 'report.json').as_posix()),\n",
    "        \"clf\": str((SAVE_DIR / 'sgd_softmax_best.joblib').as_posix()),\n",
    "        \"tfidf_word\": str((SAVE_DIR / 'tfidf_word.joblib').as_posix()),\n",
    "        \"tfidf_char\": str((SAVE_DIR / 'tfidf_char.joblib').as_posix()),\n",
    "        \"label_map\": str((SAVE_DIR / 'label_map.json').as_posix()),\n",
    "        \"inference_script\": str((SAVE_DIR / 'inference.py').as_posix()),\n",
    "    }}\n",
    "\n",
    "out2 = train_softmax_classifier_classwise(\n",
    "    data_path=\"./symptoms/training_dataset.csv\",\n",
    "    save_dir=\"./symptoms/model/\",\n",
    "    epochs=25,\n",
    "    patience=10,\n",
    "    early_stopping=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "out2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c0aa6",
   "metadata": {},
   "source": [
    "### Condition classifier inference functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_inference as mi\n",
    "\n",
    "mi.inference(user_text=\"\",first_call=True)\n",
    "mi.inference(user_text=\"\", last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0fd8f",
   "metadata": {},
   "source": [
    "### Script to generate synthetic data (1000 samples default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3333f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(8081)\n",
    "np.random.seed(8081)\n",
    "\n",
    "# ---------- Load source ----------\n",
    "src = Path(\"./symptoms/symptoms_full.csv\")\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "# Normalize headers\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "# Identify columns\n",
    "cond_id_col = next((c for c in df.columns if \"condition\" in c and \"id\" in c), None)\n",
    "cond_name_col = next((c for c in df.columns if c in (\"condition\",\"diagnosis\",\"name\",\"condition_name\")), None)\n",
    "subspec_col = next((c for c in df.columns if \"subspecial\" in c), None)\n",
    "keywords_col = next((c for c in df.columns if \"keyword\" in c), None)\n",
    "\n",
    "if not all([cond_id_col, subspec_col, keywords_col]):\n",
    "    raise RuntimeError(f\"Required columns not found. Got: {df.columns.tolist()}\")\n",
    "\n",
    "# Clean + helpers\n",
    "df = df.dropna(subset=[cond_id_col, subspec_col]).copy()\n",
    "df[cond_id_col] = df[cond_id_col].astype(int)\n",
    "\n",
    "def split_keywords(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).replace(\"|\", \",\").replace(\";\", \",\")\n",
    "    parts = [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "    seen = set(); out = []\n",
    "    for p in parts:\n",
    "        k = p.lower()\n",
    "        if k not in seen:\n",
    "            out.append(p); seen.add(k)\n",
    "    return out\n",
    "\n",
    "df[\"_kw_list\"] = df[keywords_col].apply(split_keywords)\n",
    "df[\"_cond_name\"] = df[cond_name_col] if cond_name_col else \"\"\n",
    "\n",
    "# build neighbor pools by subspecialty\n",
    "by_sub = {}\n",
    "for _, r in df.iterrows():\n",
    "    by_sub.setdefault(str(r[subspec_col]), []).append(r)\n",
    "\n",
    "# ---------- Natural-English rewriting ----------\n",
    "\n",
    "PHRASE_REWRITES = {\n",
    "    \"shortness of breath\": [\"breathlessness\", \"I get winded easily\", \"hard to catch my breath\"],\n",
    "    \"chest pain\": [\"chest tightness\", \"pressure in my chest\", \"pain in my chest\"],\n",
    "    \"pelvic pain\": [\"pelvic discomfort\", \"an ache in my lower abdomen\", \"cramping in my pelvis\"],\n",
    "    \"abdominal pain\": [\"belly pain\", \"stomach ache\", \"abdominal discomfort\"],\n",
    "    \"lower back\": [\"ache in my lower back\", \"low back soreness\", \"low back pain\"],\n",
    "    \"back pain\": [\"back ache\", \"ache in my back\", \"back soreness\"],\n",
    "    \"heavy bleeding\": [\"very heavy periods\", \"heavy flow\", \"soaking pads quickly\"],\n",
    "    \"irregular periods\": [\"my periods are irregular\", \"unpredictable periods\"],\n",
    "    \"painful periods\": [\"painful cramps\", \"cramping with my period\"],\n",
    "    \"vaginal bleeding\": [\"vaginal spotting\", \"unexpected vaginal bleeding\"],\n",
    "    \"urinary frequency\": [\"peeing often\", \"going to the bathroom a lot\"],\n",
    "    \"urinary urgency\": [\"a sudden urge to pee\", \"needing to pee right away\"],\n",
    "    \"burning urination\": [\"it burns when I pee\", \"pain when urinating\"],\n",
    "    \"dyspareunia\": [\"pain with sex\", \"sex is painful\"],\n",
    "    \"nausea\": [\"feeling queasy\", \"nauseated\"],\n",
    "    \"vomiting\": [\"throwing up\", \"vomiting\"],\n",
    "    \"diarrhea\": [\"loose stools\", \"the runs\"],\n",
    "    \"constipation\": [\"hard to pass stool\", \"constipation\"],\n",
    "    \"bloating\": [\"feeling bloated\", \"abdominal swelling\"],\n",
    "    \"fatigue\": [\"low energy\", \"feeling exhausted\", \"tired all the time\"],\n",
    "    \"dizziness\": [\"feeling lightheaded\", \"woozy spells\"],\n",
    "    \"headache\": [\"head pain\", \"throbbing headaches\", \"migraine-like pain\"],\n",
    "    \"itching\": [\"itchiness\", \"persistent itching\"],\n",
    "    \"discharge\": [\"unusual discharge\", \"unusual fluid\"],\n",
    "    \"incontinence\": [\"leaking urine\", \"urine leakage\"],\n",
    "    \"hematuria\": [\"blood in my urine\", \"pink or red urine\"],\n",
    "    \"overactive bladder\": [\"urgent and frequent urination\", \"OAB-like symptoms\"],\n",
    "    \"urinary incontinence\": [\"leaking urine\", \"stress incontinence\"],\n",
    "}\n",
    "\n",
    "TOKEN_SWAPS = {\n",
    "    \"pain\": [\"ache\", \"discomfort\", \"soreness\"],\n",
    "    \"bleeding\": [\"spotting\", \"blood loss\"],\n",
    "    \"cramps\": [\"cramping\", \"spasms\"],\n",
    "    \"swelling\": [\"bloating\", \"puffiness\"],\n",
    "    \"burning\": [\"stinging\", \"burning sensation\"],\n",
    "    \"discharge\": [\"fluid\", \"secretions\"],\n",
    "    \"urgency\": [\"urgent need\", \"urge\"],\n",
    "    \"frequency\": [\"frequent\", \"often\"],\n",
    "}\n",
    "\n",
    "DURATIONS = [\n",
    "    \"for a few days\", \"for two weeks\", \"for several months\", \"on and off\",\n",
    "    \"since yesterday\", \"recently\", \"over the past month\"\n",
    "]\n",
    "MODIFIERS = [\n",
    "    \"worse at night\", \"after exercise\", \"during my period\", \"after intercourse\",\n",
    "    \"with a low-grade fever\", \"with nausea\", \"with low back ache\",\n",
    "    \"especially when standing\", \"when I cough or sneeze\", \"worse in the morning\"\n",
    "]\n",
    "\n",
    "TEMPLATES = [\n",
    "    \"I've been dealing with {kws} {duration}, {mod}.\",\n",
    "    \"Lately I've noticed {kws} {duration}.\",\n",
    "    \"{kws} {duration}. Should I be concerned?\",\n",
    "    \"For {duration}, it's been {kws}. Any guidance?\",\n",
    "    \"I keep getting {kws} {duration}, {mod}.\",\n",
    "    \"Having {kws} {duration}. What could this be?\",\n",
    "    \"Symptoms include {kws} {duration}.\",\n",
    "    \"It's mostly {kws} {duration}, {mod}.\",\n",
    "    \"Recently it's been {kws}.\",\n",
    "    \"Persistent {kws} {duration}.\",\n",
    "]\n",
    "\n",
    "ANAT_LOCS = [\n",
    "    \"lower back\", \"pelvis\", \"pelvic area\", \"abdomen\", \"stomach\", \"chest\",\n",
    "    \"throat\", \"groin\", \"hip\", \"knee\", \"shoulder\", \"neck\", \"flank\", \"side\"\n",
    "]\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n",
    "\n",
    "def rewrite_phrase(phrase: str) -> str:\n",
    "    base = phrase.strip()\n",
    "    low = _norm(base)\n",
    "    # phrase-level\n",
    "    for k, opts in PHRASE_REWRITES.items():\n",
    "        if k in low:\n",
    "            return random.choice(opts)\n",
    "    # token-level\n",
    "    toks = base.split()\n",
    "    changed = False\n",
    "    for i, tok in enumerate(toks):\n",
    "        core = re.sub(r\"[^\\w-]\", \"\", tok.lower())\n",
    "        if core in TOKEN_SWAPS and random.random() < 0.6:\n",
    "            toks[i] = random.choice(TOKEN_SWAPS[core])\n",
    "            changed = True\n",
    "    if changed:\n",
    "        return \" \".join(toks)\n",
    "    # grammar tweak: \"<loc> pain\" -> \"pain in my <loc>\"\n",
    "    if \"pain\" in low:\n",
    "        for loc in ANAT_LOCS:\n",
    "            if loc in low:\n",
    "                return random.choice([f\"pain in my {loc}\", f\"an ache in my {loc}\", f\"{loc} pain\"])\n",
    "        return random.choice([\"ongoing pain\", \"persistent pain\", base])\n",
    "    return base\n",
    "\n",
    "def list_join(items):\n",
    "    if not items: return \"\"\n",
    "    if len(items) == 1: return items[0]\n",
    "    return \", \".join(items[:-1]) + f\", and {items[-1]}\"\n",
    "\n",
    "def choose_keywords(base_list, neighbor_pool, kmin=3, kmax=6):\n",
    "    # base_list: original row keywords\n",
    "    # neighbor_pool: list of additional keywords from neighbors\n",
    "    pool = list(dict.fromkeys(base_list + neighbor_pool))  # de-dup, keep order\n",
    "    if not pool:\n",
    "        pool = [\"fatigue\", \"bloating\"]\n",
    "    k = min(len(pool), random.randint(kmin, kmax))\n",
    "    if k == 0: k = 1\n",
    "    random.shuffle(pool)\n",
    "    return pool[:k]\n",
    "\n",
    "# Build a mapping subspecialty -> pool of other keywords\n",
    "sub_kw_pool = {}\n",
    "for sub, rows in by_sub.items():\n",
    "    kw_accum = []\n",
    "    for rr in rows:\n",
    "        kw_accum.extend(rr[\"_kw_list\"] or [])\n",
    "    # de-duplicate, keep insertion order\n",
    "    seen = set(); pooled = []\n",
    "    for w in kw_accum:\n",
    "        lw = w.lower()\n",
    "        if lw not in seen:\n",
    "            pooled.append(w); seen.add(lw)\n",
    "    sub_kw_pool[sub] = pooled\n",
    "\n",
    "# ---------- Create variants: 10 per row ----------\n",
    "variants_per_row = 10\n",
    "rows_out = []\n",
    "\n",
    "for idx, r in df.iterrows():\n",
    "    base_id = int(r[cond_id_col])\n",
    "    subspec = str(r[subspec_col])\n",
    "    base_kws = r[\"_kw_list\"] or []\n",
    "    neighbor_pool = sub_kw_pool.get(subspec, [])\n",
    "    # remove base keywords from neighbor pool to emphasize \"other sets\"\n",
    "    neighbor_pool_filtered = [w for w in neighbor_pool if _norm(w) not in {_norm(x) for x in base_kws}]\n",
    "\n",
    "    for v in range(1, variants_per_row + 1):\n",
    "        # pick a set of keywords (some from base, some from neighbor pool)\n",
    "        chosen = choose_keywords(\n",
    "            base_list=base_kws,\n",
    "            neighbor_pool=random.sample(neighbor_pool_filtered, k=min(len(neighbor_pool_filtered), random.randint(2,6))),\n",
    "            kmin=3, kmax=6\n",
    "        )\n",
    "        # rewrite some to natural English; ensure at least one exact from original base_kws remains\n",
    "        realized = []\n",
    "        had_exact_from_base = False\n",
    "        base_set_norm = {_norm(x) for x in base_kws}\n",
    "        for j, kw in enumerate(chosen):\n",
    "            if _norm(kw) in base_set_norm and random.random() < (0.55 if j == 0 else 0.4):\n",
    "                realized.append(kw)\n",
    "                had_exact_from_base = True\n",
    "            else:\n",
    "                realized.append(rewrite_phrase(kw))\n",
    "        if not had_exact_from_base and base_kws:\n",
    "            realized[0] = base_kws[0]\n",
    "\n",
    "        # build user_input\n",
    "        sentence = random.choice(TEMPLATES).format(\n",
    "            kws=list_join(realized),\n",
    "            duration=random.choice(DURATIONS),\n",
    "            mod=random.choice(MODIFIERS)\n",
    "        )\n",
    "        rows_out.append({\n",
    "            \"source_condition_id\": base_id,\n",
    "            \"source_subspecialty\": subspec,\n",
    "            \"variant_index\": v,\n",
    "            \"variant_keywords\": json.dumps(chosen),\n",
    "            \"user_input\": sentence\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "out_path = Path(\"./symptoms/keyword_sets_user_inputs.csv\")\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"source_condition_id\",\"source_subspecialty\",\"variant_index\",\"variant_keywords\",\"user_input\"], quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows_out)\n",
    "\n",
    "# Preview a slice\n",
    "preview = pd.DataFrame(rows_out).head(25)\n",
    "\n",
    "out_path.as_posix()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66511a96",
   "metadata": {},
   "source": [
    "## MOVING FORWARD IN MORNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd09bc",
   "metadata": {},
   "source": [
    "TO DO FOR LOGAN OR FOR BACKEND:\n",
    "- Create subspecialty to subspecialty_key csv file to force static index all the time.\n",
    "- Instantiate algorithm for subspecialty selection (Vector of percent): where $c_i$ is some predicted condition from user input\n",
    "$$\\text{subspecialty\\_chance}_j = \\sum_{i=0}^{k}{c_i1\\{b(i)=j\\}}, \\forall j \\in [1,s]$$\n",
    "- Instantiate algorithm for doctor selection (vector of percent): where $\\vec{d}_i$ is the experience level of each doctor on some given condition $i$\n",
    " $$\\text{doctor\\_chance}_j = ln(\\sum_{i=0}^{k}{d_j^{(c_i)}})$$\n",
    "- Consider power sharpening these\n",
    "- make SINGLE function that takes user input string and converts into subspecialty pred vector and doctor pred vector\n",
    "#### Stretch goals\n",
    "- MCPT on data\n",
    "- various visualizations of model rate of learning\n",
    "- all for the construction of \"more info\" page\n",
    "- Build lifting stuff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('./data/symptoms_full.csv')\n",
    "sspec_data = pd.read_csv('./data/sspec_key_map.csv')\n",
    "\n",
    "sympt = data.values\n",
    "\n",
    "sspec = sspec_data.values\n",
    "\n",
    "for i in range(6):\n",
    "    \n",
    "    loc = np.where(sympt[:, 1]==sspec[i,2])\n",
    "    \n",
    "    sympt[loc, 1] = sspec[i, 0]\n",
    "\n",
    "print(sympt)\n",
    "\n",
    "new_df = pd.DataFrame(data=sympt, columns=data.columns)\n",
    "new_df = new_df.drop(columns=['subspecialty'])\n",
    "new_df.to_csv('./data/symptoms_full_i.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
