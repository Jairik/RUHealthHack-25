{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f8fc6e",
   "metadata": {},
   "source": [
    "## Dev file by Logan Kelsch\n",
    "### This file is used for initial instantiation of model ideas, initial runs and tests, and temp code storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5cbb5f",
   "metadata": {},
   "source": [
    "### Depricated Train and test for Neural-Net"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 1,
   "id": "68737274",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msspec_nn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jairi/OneDrive/Desktop/Repos/RUHealthHack-25/backend/sspec_nn.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#imports\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "68737274",
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "import sspec_nn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"temp_data/train.csv\")\n",
    "\n",
    "X = data.values[:, :-1]\n",
    "y = data.values[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "model = sspec_nn.sspec_nn_train(X_train, y_train, batch_size=16, epochs=250, optimizer='adam')\n",
    "y_proba, y_pred = sspec_nn.sspec_nn_predict(model, X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.imshow(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393043b3",
   "metadata": {},
   "source": [
    "### Softmax (medical) condition classifier training function"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 11,
>>>>>>> Stashed changes
   "id": "450ebe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update function to do per-class splits guaranteeing at least 1 sample in test and 1 in val for every class.\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, top_k_accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SAVE_DIR = Path(\"./symptoms/model/\")\n",
    "\n",
    "def train_softmax_classifier_classwise(\n",
    "    data_path: str,\n",
    "    save_dir: str,\n",
<<<<<<< Updated upstream
    "    random_state: int=42,\n",
=======
    "    random_state: int=None,\n",
>>>>>>> Stashed changes
    "    epochs: int=6,\n",
    "    batch_size: int=128,\n",
    "    lr_alpha: float=1e-5,\n",
    "    learning_rate: str=\"optimal\",\n",
    "    eta0: float=0.0,\n",
    "    ngram_range_word=(1,2),\n",
    "    ngram_range_char=(3,5),\n",
    "    max_features_word: int=40000,\n",
    "    max_features_char: int=50000,\n",
    "    min_df: int=2,\n",
    "    early_stopping: bool=True,\n",
    "    patience: int=2,\n",
    "    verbose: bool=True\n",
    "):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    data_path = Path(data_path); save_dir = Path(save_dir); save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df = pd.read_csv(data_path)\n",
    "    texts_all = df[\"user_input\"].astype(str).to_numpy()\n",
    "    y_all = df[\"target_condition_id\"].astype(int).to_numpy()\n",
    "\n",
    "    # keep classes with >=3\n",
    "    uniq, counts = np.unique(y_all, return_counts=True)\n",
    "    keep = uniq[counts >= 3]\n",
    "    mask = np.isin(y_all, keep)\n",
    "    texts = texts_all[mask]; y = y_all[mask]\n",
    "\n",
    "    # map to indices\n",
    "    unique_ids = np.array(sorted(np.unique(y).tolist()), dtype=int)\n",
    "    id_to_idx = {cid: i for i, cid in enumerate(unique_ids)}\n",
    "    y_idx = np.array([id_to_idx[int(v)] for v in y], dtype=int)\n",
    "\n",
    "    # build per-class indices and split: 1 to test, 1 to val, rest to train\n",
    "    by_class = {c: np.where(y_idx == c)[0] for c in range(len(unique_ids))}\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for c, idxs in by_class.items():\n",
    "        idxs = idxs.copy()\n",
    "        rng.shuffle(idxs)\n",
    "        if len(idxs) >= 3:\n",
    "            test_idx.append(idxs[0])\n",
    "            val_idx.append(idxs[1])\n",
    "            train_idx.extend(idxs[2:])\n",
    "        else:\n",
    "            # shouldn't happen (we filtered), but fallback: 1 test, rest train\n",
    "            test_idx.append(idxs[0])\n",
    "            train_idx.extend(idxs[1:])\n",
    "\n",
    "    X_train_text = texts[train_idx].tolist()\n",
    "    X_val_text = texts[val_idx].tolist()\n",
    "    X_test_text = texts[test_idx].tolist()\n",
    "    y_train_idx = y_idx[train_idx]\n",
    "    y_val_idx = y_idx[val_idx]\n",
    "    y_test_idx = y_idx[test_idx]\n",
    "\n",
    "    # vectorizers\n",
    "    v_word = TfidfVectorizer(analyzer=\"word\", ngram_range=ngram_range_word, min_df=min_df, max_features=max_features_word, lowercase=True)\n",
    "    v_char = TfidfVectorizer(analyzer=\"char\", ngram_range=ngram_range_char, min_df=min_df, max_features=max_features_char, lowercase=True)\n",
    "    Xw_tr = v_word.fit_transform(X_train_text); Xc_tr = v_char.fit_transform(X_train_text); X_tr = hstack([Xw_tr, Xc_tr], format=\"csr\")\n",
    "    Xw_va = v_word.transform(X_val_text); Xc_va = v_char.transform(X_val_text); X_va = hstack([Xw_va, Xc_va], format=\"csr\")\n",
    "    Xw_te = v_word.transform(X_test_text); Xc_te = v_char.transform(X_test_text); X_te = hstack([Xw_te, Xc_te], format=\"csr\")\n",
    "\n",
    "    # class weights\n",
    "    classes, counts_tr = np.unique(y_train_idx, return_counts=True)\n",
    "    freq = counts_tr / counts_tr.sum()\n",
    "    w = 1.0 / np.maximum(freq, 1e-12)\n",
    "    weight_map = {int(c): w[i]*(len(w)/w.sum()) for i,c in enumerate(classes)}\n",
    "\n",
    "    clf = SGDClassifier(loss=\"log_loss\", alpha=lr_alpha, learning_rate=learning_rate, eta0=eta0, random_state=random_state)\n",
    "    classes_all = np.arange(len(unique_ids), dtype=int)\n",
    "    n_train = X_tr.shape[0]\n",
    "    best_val_f1 = -1.0; best_epoch = -1\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        order = np.arange(n_train); rng.shuffle(order)\n",
    "        for start in range(0, n_train, batch_size):\n",
    "            end = min(start + batch_size, n_train)\n",
    "            idx = order[start:end]\n",
    "            xb = X_tr[idx]; yb = y_train_idx[idx]\n",
    "            sw = np.array([weight_map[int(c)] for c in yb], dtype=float)\n",
    "            if epoch == 1 and start == 0:\n",
    "                clf.partial_fit(xb, yb, classes=classes_all, sample_weight=sw)\n",
    "            else:\n",
    "                clf.partial_fit(xb, yb, sample_weight=sw)\n",
    "        # validation\n",
    "        proba_va = clf.predict_proba(X_va); pred_va = np.argmax(proba_va, axis=1)\n",
    "        acc = accuracy_score(y_val_idx, pred_va)\n",
    "        macro_f1 = f1_score(y_val_idx, pred_va, average=\"macro\")\n",
    "        top3 = top_k_accuracy_score(y_val_idx, proba_va, k=min(3, proba_va.shape[1]))\n",
    "        top5 = top_k_accuracy_score(y_val_idx, proba_va, k=min(5, proba_va.shape[1]))\n",
    "        if verbose:\n",
    "            print(f\"[Epoch {epoch}] val_acc={acc:.3f} macroF1={macro_f1:.3f} top3={top3:.3f} top5={top5:.3f}\")\n",
    "        if macro_f1 > best_val_f1:\n",
    "            best_val_f1 = macro_f1; best_epoch = epoch\n",
    "            dump(clf, SAVE_DIR / \"sgd_softmax_best.joblib\")\n",
    "            dump(v_word, SAVE_DIR / \"tfidf_word.joblib\")\n",
    "            dump(v_char, SAVE_DIR / \"tfidf_char.joblib\")\n",
    "            with open(SAVE_DIR / \"label_map.json\", \"w\") as f:\n",
    "                json.dump([int(x) for x in unique_ids], f)\n",
    "        if early_stopping and (epoch - best_epoch) >= patience:\n",
    "            if verbose:\n",
    "                print(f\"[EarlyStopping] no improvement for {patience} epochs (best epoch {best_epoch}). Stop.\")\n",
    "            break\n",
    "\n",
    "    # test eval\n",
    "    clf = load(SAVE_DIR / \"sgd_softmax_best.joblib\")\n",
    "    proba_te = clf.predict_proba(X_te); pred_te = np.argmax(proba_te, axis=1)\n",
    "    acc_te = accuracy_score(y_test_idx, pred_te)\n",
    "    macro_f1_te = f1_score(y_test_idx, pred_te, average=\"macro\")\n",
    "    micro_f1_te = f1_score(y_test_idx, pred_te, average=\"micro\")\n",
    "    top3_te = top_k_accuracy_score(y_test_idx, proba_te, k=min(3, proba_te.shape[1]))\n",
    "    top5_te = top_k_accuracy_score(y_test_idx, proba_te, k=min(5, proba_te.shape[1]))\n",
    "\n",
    "    report = {\n",
    "        \"classes_kept_ge3\": int(len(unique_ids)),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"test_accuracy\": float(acc_te),\n",
    "        \"test_macro_f1\": float(macro_f1_te),\n",
    "        \"test_micro_f1\": float(micro_f1_te),\n",
    "        \"test_top3_accuracy\": float(top3_te),\n",
    "        \"test_top5_accuracy\": float(top5_te),\n",
    "    }\n",
    "    with open(SAVE_DIR / \"report.json\", \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    # inference script\n",
    "    infer_code = f\"\"\"#!/usr/bin/env python3\n",
    "import json, sys\n",
    "from joblib import load\n",
    "from scipy.sparse import hstack\n",
    "word = load(r\"{(SAVE_DIR / 'tfidf_word.joblib').as_posix()}\")\n",
    "char = load(r\"{(SAVE_DIR / 'tfidf_char.joblib').as_posix()}\")\n",
    "clf = load(r\"{(SAVE_DIR / 'sgd_softmax_best.joblib').as_posix()}\")\n",
    "with open(r\"{(SAVE_DIR / 'label_map.json').as_posix()}\") as f:\n",
    "    label_map = json.load(f)\n",
    "def predict_topk(text, k=5):\n",
    "    X = hstack([word.transform([text]), char.transform([text])], format='csr')\n",
    "    p = clf.predict_proba(X)[0]\n",
    "    idxs = p.argsort()[::-1][:k]\n",
    "    return [(int(label_map[i]), float(p[i])) for i in idxs]\n",
    "if __name__ == \"__main__\":\n",
    "    txt = \" \".join(sys.argv[1:]) or \"pelvic pressure and heavy flow\"\n",
    "    print(predict_topk(txt, k=5))\n",
    "\"\"\"\n",
    "    (SAVE_DIR / \"inference.py\").write_text(infer_code)\n",
    "\n",
    "    return {\"report\": report, \"paths\": {\n",
    "        \"report\": str((SAVE_DIR / 'report.json').as_posix()),\n",
    "        \"clf\": str((SAVE_DIR / 'sgd_softmax_best.joblib').as_posix()),\n",
    "        \"tfidf_word\": str((SAVE_DIR / 'tfidf_word.joblib').as_posix()),\n",
    "        \"tfidf_char\": str((SAVE_DIR / 'tfidf_char.joblib').as_posix()),\n",
    "        \"label_map\": str((SAVE_DIR / 'label_map.json').as_posix()),\n",
    "        \"inference_script\": str((SAVE_DIR / 'inference.py').as_posix()),\n",
<<<<<<< Updated upstream
    "    }}\n",
    "\n",
    "out2 = train_softmax_classifier_classwise(\n",
    "    data_path=\"./symptoms/training_dataset.csv\",\n",
    "    save_dir=\"./symptoms/model/\",\n",
=======
    "    }}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d14c0944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] val_acc=0.421 macroF1=0.342 top3=0.664 top5=0.686\n",
      "[Epoch 2] val_acc=0.479 macroF1=0.390 top3=0.686 top5=0.707\n",
      "[Epoch 3] val_acc=0.529 macroF1=0.440 top3=0.693 top5=0.736\n",
      "[Epoch 4] val_acc=0.464 macroF1=0.396 top3=0.729 top5=0.771\n",
      "[Epoch 5] val_acc=0.543 macroF1=0.467 top3=0.750 top5=0.793\n",
      "[Epoch 6] val_acc=0.529 macroF1=0.448 top3=0.807 top5=0.843\n",
      "[Epoch 7] val_acc=0.550 macroF1=0.481 top3=0.850 top5=0.864\n",
      "[Epoch 8] val_acc=0.571 macroF1=0.495 top3=0.857 top5=0.879\n",
      "[Epoch 9] val_acc=0.571 macroF1=0.499 top3=0.857 top5=0.886\n",
      "[Epoch 10] val_acc=0.586 macroF1=0.515 top3=0.879 top5=0.900\n",
      "[Epoch 11] val_acc=0.586 macroF1=0.510 top3=0.879 top5=0.907\n",
      "[Epoch 12] val_acc=0.586 macroF1=0.511 top3=0.893 top5=0.907\n",
      "[Epoch 13] val_acc=0.586 macroF1=0.512 top3=0.893 top5=0.907\n",
      "[Epoch 14] val_acc=0.593 macroF1=0.520 top3=0.907 top5=0.921\n",
      "[Epoch 15] val_acc=0.586 macroF1=0.510 top3=0.907 top5=0.921\n",
      "[Epoch 16] val_acc=0.586 macroF1=0.512 top3=0.900 top5=0.921\n",
      "[Epoch 17] val_acc=0.600 macroF1=0.529 top3=0.900 top5=0.929\n",
      "[Epoch 18] val_acc=0.593 macroF1=0.524 top3=0.900 top5=0.936\n",
      "[Epoch 19] val_acc=0.600 macroF1=0.530 top3=0.907 top5=0.936\n",
      "[Epoch 20] val_acc=0.593 macroF1=0.525 top3=0.907 top5=0.936\n",
      "[Epoch 21] val_acc=0.593 macroF1=0.524 top3=0.907 top5=0.936\n",
      "[Epoch 22] val_acc=0.600 macroF1=0.538 top3=0.907 top5=0.936\n",
      "[Epoch 23] val_acc=0.593 macroF1=0.524 top3=0.900 top5=0.936\n",
      "[Epoch 24] val_acc=0.614 macroF1=0.546 top3=0.900 top5=0.950\n",
      "[Epoch 25] val_acc=0.607 macroF1=0.537 top3=0.900 top5=0.950\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "out2 = train_softmax_classifier_classwise(\n",
    "    data_path=\"./data/training_dataset.csv\",\n",
    "    save_dir=\"./model/training/\",\n",
>>>>>>> Stashed changes
    "    epochs=25,\n",
    "    patience=10,\n",
    "    early_stopping=True,\n",
    "    verbose=True\n",
<<<<<<< Updated upstream
    ")\n",
    "\n",
    "out2\n"
=======
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9dbd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] val_acc=0.841 macroF1=0.854 top3=0.946 top5=0.982\n",
      "[Epoch 2] val_acc=0.927 macroF1=0.930 top3=0.980 top5=0.998\n",
      "[Epoch 3] val_acc=0.956 macroF1=0.959 top3=0.997 top5=1.000\n",
      "[Epoch 4] val_acc=0.960 macroF1=0.963 top3=1.000 top5=1.000\n",
      "[Epoch 5] val_acc=0.954 macroF1=0.959 top3=1.000 top5=1.000\n",
      "[Epoch 6] val_acc=0.961 macroF1=0.965 top3=1.000 top5=1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'report': {'n_classes': 6,\n",
       "  'best_epoch': 6,\n",
       "  'test_accuracy': 1.0,\n",
       "  'test_macro_f1': 1.0,\n",
       "  'test_micro_f1': 1.0,\n",
       "  'test_top3_accuracy': 1.0,\n",
       "  'test_top5_accuracy': 1.0,\n",
       "  'val_frac': 0.9,\n",
       "  'test_frac': 0.01},\n",
       " 'paths': {'report': 'symptoms/model/report.json',\n",
       "  'clf': 'symptoms/model/sgd_softmax_best.joblib',\n",
       "  'tfidf_word': 'symptoms/model/tfidf_word.joblib',\n",
       "  'tfidf_char': 'symptoms/model/tfidf_char.joblib',\n",
       "  'label_map': 'symptoms/model/label_map.json',\n",
       "  'per_class_metrics': 'symptoms/model/per_class_metrics.csv',\n",
       "  'inference_script': 'symptoms/model/inference.py'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, top_k_accuracy_score, precision_score, recall_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def train_softmax_classifier_classwise(\n",
    "    data_path: str,\n",
    "    save_dir: str,\n",
    "    random_state: int=None,\n",
    "    epochs: int=6,\n",
    "    batch_size: int=128,\n",
    "    lr_alpha: float=1e-5,\n",
    "    learning_rate: str=\"optimal\",\n",
    "    eta0: float=0.0,\n",
    "    ngram_range_word=(1,2),\n",
    "    ngram_range_char=(3,5),\n",
    "    max_features_word: int=40000,\n",
    "    max_features_char: int=50000,\n",
    "    min_df: int=2,\n",
    "    early_stopping: bool=True,\n",
    "    patience: int=2,\n",
    "    verbose: bool=True,\n",
    "    val_frac: float=0.15,          # <— % of each class to validation\n",
    "    test_frac: float=0.15          # <— % of each class to test\n",
    "):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    data_path = Path(data_path); save_dir = Path(save_dir); save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # X = text column (assumes 'user_input' exists)\n",
    "    texts_all = df[\"user_input\"].astype(str).to_numpy()\n",
    "\n",
    "    # y = SECOND COLUMN (6 strings); binarize (one-hot) + label-encode to 0..5\n",
    "    target_col = df.columns[1]\n",
    "    y_str_all = df.iloc[:, 1].astype(str).to_numpy()\n",
    "\n",
    "    # keep classes with >=3 samples (for per-class val/test safety)\n",
    "    uniq, counts = np.unique(y_str_all, return_counts=True)\n",
    "    keep = set(uniq[counts >= 3])\n",
    "    mask = np.array([s in keep for s in y_str_all], dtype=bool)\n",
    "    texts = texts_all[mask]\n",
    "    y_str = y_str_all[mask]\n",
    "\n",
    "    # label-encode to indices and also build one-hot for analysis\n",
    "    classes_sorted = np.array(sorted(np.unique(y_str)), dtype=object)\n",
    "    str2idx = {s:i for i,s in enumerate(classes_sorted)}\n",
    "    y_idx = np.array([str2idx[s] for s in y_str], dtype=int)\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=False, dtype=np.float32)\n",
    "    Y_onehot = ohe.fit_transform(y_idx.reshape(-1,1))  # not used by SGDClassifier, but kept for analysis\n",
    "\n",
    "    # per-class split with val/test fractions (>=1 each)\n",
    "    by_class = {c: np.where(y_idx == c)[0] for c in range(len(classes_sorted))}\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for c, idxs in by_class.items():\n",
    "        idxs = idxs.copy()\n",
    "        rng.shuffle(idxs)\n",
    "        n = len(idxs)\n",
    "        n_test = max(1, int(round(test_frac * n)))\n",
    "        n_val  = max(1, int(round(val_frac  * n)))\n",
    "        # ensure room for train\n",
    "        if n_test + n_val >= n:\n",
    "            # leave at least 1 for train if possible\n",
    "            if n >= 3:\n",
    "                n_test = 1\n",
    "                n_val  = 1\n",
    "            else:\n",
    "                # fallback: 1 test, rest train (shouldn't happen due to >=3 filter)\n",
    "                n_test = 1\n",
    "                n_val  = 0\n",
    "        test_idx.extend(idxs[:n_test])\n",
    "        val_idx.extend(idxs[n_test:n_test+n_val])\n",
    "        train_idx.extend(idxs[n_test+n_val:])\n",
    "\n",
    "    X_train_text = texts[train_idx].tolist()\n",
    "    X_val_text   = texts[val_idx].tolist()\n",
    "    X_test_text  = texts[test_idx].tolist()\n",
    "    y_train_idx  = y_idx[train_idx]\n",
    "    y_val_idx    = y_idx[val_idx]\n",
    "    y_test_idx   = y_idx[test_idx]\n",
    "\n",
    "    # vectorizers\n",
    "    v_word = TfidfVectorizer(analyzer=\"word\", ngram_range=ngram_range_word, min_df=min_df, max_features=max_features_word, lowercase=True)\n",
    "    v_char = TfidfVectorizer(analyzer=\"char\", ngram_range=ngram_range_char, min_df=min_df, max_features=max_features_char, lowercase=True)\n",
    "    Xw_tr = v_word.fit_transform(X_train_text); Xc_tr = v_char.fit_transform(X_train_text); X_tr = hstack([Xw_tr, Xc_tr], format=\"csr\")\n",
    "    Xw_va = v_word.transform(X_val_text);       Xc_va = v_char.transform(X_val_text);       X_va = hstack([Xw_va, Xc_va], format=\"csr\")\n",
    "    Xw_te = v_word.transform(X_test_text);      Xc_te = v_char.transform(X_test_text);      X_te = hstack([Xw_te, Xc_te], format=\"csr\")\n",
    "\n",
    "    # class weights (inverse-frequency on train)\n",
    "    classes_train, counts_tr = np.unique(y_train_idx, return_counts=True)\n",
    "    freq = counts_tr / counts_tr.sum()\n",
    "    w = 1.0 / np.maximum(freq, 1e-12)\n",
    "    weight_map = {int(c): w[i]*(len(w)/w.sum()) for i,c in enumerate(classes_train)}\n",
    "\n",
    "    clf = SGDClassifier(loss=\"log_loss\", alpha=lr_alpha, learning_rate=learning_rate, eta0=eta0, random_state=random_state)\n",
    "    classes_all = np.arange(len(classes_sorted), dtype=int)\n",
    "    n_train = X_tr.shape[0]\n",
    "    best_val_f1 = -1.0; best_epoch = -1\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        order = np.arange(n_train); rng.shuffle(order)\n",
    "        for start in range(0, n_train, batch_size):\n",
    "            end = min(start + batch_size, n_train)\n",
    "            idx = order[start:end]\n",
    "            xb = X_tr[idx]; yb = y_train_idx[idx]\n",
    "            sw = np.array([weight_map[int(c)] for c in yb], dtype=float)\n",
    "            if epoch == 1 and start == 0:\n",
    "                clf.partial_fit(xb, yb, classes=classes_all, sample_weight=sw)\n",
    "            else:\n",
    "                clf.partial_fit(xb, yb, sample_weight=sw)\n",
    "\n",
    "        # validation\n",
    "        proba_va = clf.predict_proba(X_va); pred_va = np.argmax(proba_va, axis=1)\n",
    "        acc = accuracy_score(y_val_idx, pred_va)\n",
    "        macro_f1 = f1_score(y_val_idx, pred_va, average=\"macro\")\n",
    "        top3 = top_k_accuracy_score(y_val_idx, proba_va, k=min(3, proba_va.shape[1]))\n",
    "        top5 = top_k_accuracy_score(y_val_idx, proba_va, k=min(5, proba_va.shape[1]))\n",
    "        if verbose:\n",
    "            print(f\"[Epoch {epoch}] val_acc={acc:.3f} macroF1={macro_f1:.3f} top3={top3:.3f} top5={top5:.3f}\")\n",
    "        if macro_f1 > best_val_f1:\n",
    "            best_val_f1 = macro_f1; best_epoch = epoch\n",
    "            dump(clf, SAVE_DIR / \"sgd_softmax_best.joblib\")\n",
    "            dump(v_word, SAVE_DIR / \"tfidf_word.joblib\")\n",
    "            dump(v_char, SAVE_DIR / \"tfidf_char.joblib\")\n",
    "            # save label map: index -> class string\n",
    "            with open(SAVE_DIR / \"label_map.json\", \"w\") as f:\n",
    "                json.dump(list(map(str, classes_sorted.tolist())), f)\n",
    "        if early_stopping and (epoch - best_epoch) >= patience:\n",
    "            if verbose:\n",
    "                print(f\"[EarlyStopping] no improvement for {patience} epochs (best epoch {best_epoch}). Stop.\")\n",
    "            break\n",
    "\n",
    "    # test eval\n",
    "    clf = load(SAVE_DIR / \"sgd_softmax_best.joblib\")\n",
    "    proba_te = clf.predict_proba(X_te); pred_te = np.argmax(proba_te, axis=1)\n",
    "    acc_te = accuracy_score(y_test_idx, pred_te)\n",
    "    macro_f1_te = f1_score(y_test_idx, pred_te, average=\"macro\")\n",
    "    micro_f1_te = f1_score(y_test_idx, pred_te, average=\"micro\")\n",
    "    top3_te = top_k_accuracy_score(y_test_idx, proba_te, k=min(3, proba_te.shape[1]))\n",
    "    top5_te = top_k_accuracy_score(y_test_idx, proba_te, k=min(5, proba_te.shape[1]))\n",
    "\n",
    "    # per-class metrics (precision + \"class accuracy\" == recall)\n",
    "    precisions = precision_score(y_test_idx, pred_te, average=None, labels=np.arange(len(classes_sorted)))\n",
    "    recalls    = recall_score(y_test_idx, pred_te, average=None, labels=np.arange(len(classes_sorted)))\n",
    "    per_class_df = pd.DataFrame({\n",
    "        \"class_idx\": np.arange(len(classes_sorted)),\n",
    "        \"class_name\": classes_sorted,\n",
    "        \"precision\": precisions,\n",
    "        \"class_accuracy(recall)\": recalls,\n",
    "        \"support\": np.bincount(y_test_idx, minlength=len(classes_sorted))\n",
    "    })\n",
    "    per_class_df.to_csv(SAVE_DIR / \"per_class_metrics.csv\", index=False)\n",
    "\n",
    "    report = {\n",
    "        \"n_classes\": int(len(classes_sorted)),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"test_accuracy\": float(acc_te),\n",
    "        \"test_macro_f1\": float(macro_f1_te),\n",
    "        \"test_micro_f1\": float(micro_f1_te),\n",
    "        \"test_top3_accuracy\": float(top3_te),\n",
    "        \"test_top5_accuracy\": float(top5_te),\n",
    "        \"val_frac\": float(val_frac),\n",
    "        \"test_frac\": float(test_frac),\n",
    "    }\n",
    "    with open(SAVE_DIR / \"report.json\", \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    # inference script (uses class strings)\n",
    "    infer_code = f\"\"\"#!/usr/bin/env python3\n",
    "import json, sys\n",
    "from joblib import load\n",
    "from scipy.sparse import hstack\n",
    "word = load(r\"{(SAVE_DIR / 'tfidf_word.joblib').as_posix()}\")\n",
    "char = load(r\"{(SAVE_DIR / 'tfidf_char.joblib').as_posix()}\")\n",
    "clf = load(r\"{(SAVE_DIR / 'sgd_softmax_best.joblib').as_posix()}\")\n",
    "with open(r\"{(SAVE_DIR / 'label_map.json').as_posix()}\") as f:\n",
    "    label_map = json.load(f)  # index -> class string\n",
    "def predict_topk(text, k=5):\n",
    "    X = hstack([word.transform([text]), char.transform([text])], format='csr')\n",
    "    p = clf.predict_proba(X)[0]\n",
    "    idxs = p.argsort()[::-1][:k]\n",
    "    return [(label_map[i], float(p[i])) for i in idxs]\n",
    "if __name__ == \"__main__\":\n",
    "    txt = \" \".join(sys.argv[1:]) or \"pelvic pressure and heavy flow\"\n",
    "    print(predict_topk(txt, k=5))\n",
    "\"\"\"\n",
    "    (SAVE_DIR / \"inference.py\").write_text(infer_code)\n",
    "\n",
    "    return {\n",
    "        \"report\": report,\n",
    "        \"paths\": {\n",
    "            \"report\": str((SAVE_DIR / 'report.json').as_posix()),\n",
    "            \"clf\": str((SAVE_DIR / 'sgd_softmax_best.joblib').as_posix()),\n",
    "            \"tfidf_word\": str((SAVE_DIR / 'tfidf_word.joblib').as_posix()),\n",
    "            \"tfidf_char\": str((SAVE_DIR / 'tfidf_char.joblib').as_posix()),\n",
    "            \"label_map\": str((SAVE_DIR / 'label_map.json').as_posix()),\n",
    "            \"per_class_metrics\": str((SAVE_DIR / 'per_class_metrics.csv').as_posix()),\n",
    "            \"inference_script\": str((SAVE_DIR / 'inference.py').as_posix()),\n",
    "        }\n",
    "    }\n",
    "\n",
    "train_softmax_classifier_classwise(\"./data/training_dataset.csv\",\"./model/training\",ngram_range_char=(1,1),ngram_range_word=(1,1),max_features_char=50,max_features_word=50, val_frac=.9, test_frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c89ec67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] val_acc=0.800 macroF1=0.814 top3=0.906 top5=0.987\n",
      "[Epoch 2] val_acc=0.884 macroF1=0.889 top3=0.994 top5=0.997\n",
      "[Epoch 3] val_acc=0.912 macroF1=0.918 top3=0.993 top5=0.998\n",
      "[Epoch 4] val_acc=0.902 macroF1=0.912 top3=0.994 top5=0.999\n",
      "[Epoch 5] val_acc=0.907 macroF1=0.910 top3=0.990 top5=0.998\n",
      "[Epoch 6] val_acc=0.925 macroF1=0.931 top3=0.993 top5=0.998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'report': {'n_classes': 6,\n",
       "  'best_epoch': 6,\n",
       "  'test_accuracy': 1.0,\n",
       "  'test_macro_f1': 1.0,\n",
       "  'test_micro_f1': 1.0,\n",
       "  'test_top3_accuracy': 1.0,\n",
       "  'test_top5_accuracy': 1.0,\n",
       "  'val_frac': 0.9,\n",
       "  'test_frac': 0.01},\n",
       " 'paths': {'report': 'symptoms/model/report.json',\n",
       "  'clf': 'symptoms/model/sgd_softmax_best.joblib',\n",
       "  'tfidf_word': 'symptoms/model/tfidf_word.joblib',\n",
       "  'tfidf_char': 'symptoms/model/tfidf_char.joblib',\n",
       "  'label_map': 'symptoms/model/label_map.json',\n",
       "  'per_class_metrics': 'symptoms/model/per_class_metrics.csv',\n",
       "  'inference_script': 'symptoms/model/inference.py'}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, top_k_accuracy_score,\n",
    "    precision_score, recall_score\n",
    ")\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def train_softmax_classifier_progress(\n",
    "    data_path: str,\n",
    "    save_dir: str,\n",
    "    random_state: int=None,\n",
    "    epochs: int=6,\n",
    "    batch_size: int=128,\n",
    "    lr_alpha: float=1e-5,\n",
    "    learning_rate: str=\"optimal\",\n",
    "    eta0: float=0.0,\n",
    "    ngram_range_word=(1,2),\n",
    "    ngram_range_char=(3,5),\n",
    "    max_features_word: int=40000,\n",
    "    max_features_char: int=50000,\n",
    "    min_df: int=2,\n",
    "    early_stopping: bool=True,\n",
    "    patience: int=2,\n",
    "    verbose: bool=True,\n",
    "    val_frac: float=0.15,\n",
    "    test_frac: float=0.15\n",
    "):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    data_path = Path(data_path)\n",
    "    save_dir = Path(save_dir); save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # X: text; y: SECOND CSV COLUMN (strings)\n",
    "    df = pd.read_csv(data_path)\n",
    "    texts_all = df[\"user_input\"].astype(str).to_numpy()\n",
    "    y_str_all = df.iloc[:, 1].astype(str).to_numpy()\n",
    "\n",
    "    # keep classes with >=3 (so we can place ≥1 in val & test)\n",
    "    uniq, counts = np.unique(y_str_all, return_counts=True)\n",
    "    keep = set(uniq[counts >= 3])\n",
    "    mask = np.array([s in keep for s in y_str_all], dtype=bool)\n",
    "    texts = texts_all[mask]\n",
    "    y_str = y_str_all[mask]\n",
    "\n",
    "    # label-encode class strings\n",
    "    classes_sorted = np.array(sorted(np.unique(y_str)), dtype=object)\n",
    "    str2idx = {s:i for i,s in enumerate(classes_sorted)}\n",
    "    y_idx = np.array([str2idx[s] for s in y_str], dtype=int)\n",
    "    n_classes = len(classes_sorted)\n",
    "\n",
    "    # per-class split with ≥1 val/test each (rest to train)\n",
    "    by_class = {c: np.where(y_idx == c)[0] for c in range(n_classes)}\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for c, idxs in by_class.items():\n",
    "        idxs = idxs.copy()\n",
    "        rng.shuffle(idxs)\n",
    "        n = len(idxs)\n",
    "        n_test = max(1, int(round(test_frac * n)))\n",
    "        n_val  = max(1, int(round(val_frac  * n)))\n",
    "        if n_test + n_val >= n:\n",
    "            if n >= 3:\n",
    "                n_test, n_val = 1, 1\n",
    "            else:\n",
    "                n_test, n_val = 1, 0\n",
    "        test_idx.extend(idxs[:n_test])\n",
    "        val_idx.extend(idxs[n_test:n_test+n_val])\n",
    "        train_idx.extend(idxs[n_test+n_val:])\n",
    "\n",
    "    # materialize splits\n",
    "    X_train_text = texts[train_idx].tolist()\n",
    "    X_val_text   = texts[val_idx].tolist()\n",
    "    X_test_text  = texts[test_idx].tolist()\n",
    "    y_train_idx  = y_idx[train_idx]\n",
    "    y_val_idx    = y_idx[val_idx]\n",
    "    y_test_idx   = y_idx[test_idx]\n",
    "\n",
    "    # sanity (no overlap)\n",
    "    assert set(train_idx).isdisjoint(val_idx)\n",
    "    assert set(train_idx).isdisjoint(test_idx)\n",
    "    assert set(val_idx).isdisjoint(test_idx)\n",
    "\n",
    "    # vectorizers\n",
    "    v_word = TfidfVectorizer(analyzer=\"word\", ngram_range=ngram_range_word,\n",
    "                             min_df=min_df, max_features=max_features_word, lowercase=True)\n",
    "    v_char = TfidfVectorizer(analyzer=\"char\", ngram_range=ngram_range_char,\n",
    "                             min_df=min_df, max_features=max_features_char, lowercase=True)\n",
    "    Xw_tr = v_word.fit_transform(X_train_text); Xc_tr = v_char.fit_transform(X_train_text); X_tr = hstack([Xw_tr, Xc_tr], format=\"csr\")\n",
    "    Xw_va = v_word.transform(X_val_text);       Xc_va = v_char.transform(X_val_text);       X_va = hstack([Xw_va, Xc_va], format=\"csr\")\n",
    "    Xw_te = v_word.transform(X_test_text);      Xc_te = v_char.transform(X_test_text);      X_te = hstack([Xw_te, Xc_te], format=\"csr\")\n",
    "\n",
    "    # class weights on train (inv-freq, normalized)\n",
    "    cls_tr, cnt_tr = np.unique(y_train_idx, return_counts=True)\n",
    "    freq = cnt_tr / cnt_tr.sum()\n",
    "    w = 1.0 / np.maximum(freq, 1e-12)\n",
    "    weight_map = {int(c): w[i]*(len(w)/w.sum()) for i,c in enumerate(cls_tr)}\n",
    "\n",
    "    # model\n",
    "    clf = SGDClassifier(loss=\"log_loss\", alpha=lr_alpha,\n",
    "                        learning_rate=learning_rate, eta0=eta0,\n",
    "                        random_state=random_state)\n",
    "    classes_all = np.arange(n_classes, dtype=int)\n",
    "    n_train = X_tr.shape[0]\n",
    "\n",
    "    # histories\n",
    "    hist_overall_acc = []\n",
    "    hist_macro_f1    = []\n",
    "    hist_perclass_precision = []  # list of (n_classes,) per epoch\n",
    "    hist_perclass_recall    = []  # list of (n_classes,) per epoch\n",
    "\n",
    "    best_val_f1 = -1.0; best_epoch = -1\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        order = np.arange(n_train); rng.shuffle(order)\n",
    "        for start in range(0, n_train, batch_size):\n",
    "            end = min(start + batch_size, n_train)\n",
    "            idx = order[start:end]\n",
    "            xb = X_tr[idx]; yb = y_train_idx[idx]\n",
    "            sw = np.array([weight_map[int(c)] for c in yb], dtype=float)\n",
    "            if epoch == 1 and start == 0:\n",
    "                clf.partial_fit(xb, yb, classes=classes_all, sample_weight=sw)\n",
    "            else:\n",
    "                clf.partial_fit(xb, yb, sample_weight=sw)\n",
    "\n",
    "        # validation snapshot\n",
    "        proba_va = clf.predict_proba(X_va); pred_va = np.argmax(proba_va, axis=1)\n",
    "        acc = accuracy_score(y_val_idx, pred_va)\n",
    "        macro_f1 = f1_score(y_val_idx, pred_va, average=\"macro\")\n",
    "        p_c = precision_score(y_val_idx, pred_va, average=None, labels=np.arange(n_classes), zero_division=0.0)\n",
    "        r_c = recall_score(   y_val_idx, pred_va, average=None, labels=np.arange(n_classes), zero_division=0.0)\n",
    "\n",
    "        hist_overall_acc.append(acc)\n",
    "        hist_macro_f1.append(macro_f1)\n",
    "        hist_perclass_precision.append(p_c)\n",
    "        hist_perclass_recall.append(r_c)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[Epoch {epoch}] val_acc={acc:.3f} macroF1={macro_f1:.3f}\")\n",
    "\n",
    "        if macro_f1 > best_val_f1:\n",
    "            best_val_f1 = macro_f1; best_epoch = epoch\n",
    "            dump(clf, save_dir / \"sgd_softmax_best.joblib\")\n",
    "            dump(v_word, save_dir / \"tfidf_word.joblib\")\n",
    "            dump(v_char, save_dir / \"tfidf_char.joblib\")\n",
    "            with open(save_dir / \"label_map.json\", \"w\") as f:\n",
    "                json.dump(list(map(str, classes_sorted.tolist())), f)\n",
    "\n",
    "        if early_stopping and (epoch - best_epoch) >= patience:\n",
    "            if verbose:\n",
    "                print(f\"[EarlyStopping] no improvement for {patience} epochs (best epoch {best_epoch}). Stop.\")\n",
    "            break\n",
    "\n",
    "    # histories → arrays\n",
    "    hist_perclass_precision = np.vstack(hist_perclass_precision) if hist_perclass_precision else np.zeros((0, n_classes))\n",
    "    hist_perclass_recall    = np.vstack(hist_perclass_recall)    if hist_perclass_recall else np.zeros((0, n_classes))\n",
    "    epochs_axis = np.arange(1, len(hist_overall_acc)+1)\n",
    "\n",
    "    # === per-class arrays (precision & recall) each length = epochs ===\n",
    "    ran_epochs = len(hist_overall_acc)\n",
    "    def _pad(arr_list, target_len):\n",
    "        return arr_list + [None] * (target_len - len(arr_list)) if len(arr_list) < target_len else arr_list[:target_len]\n",
    "\n",
    "    precision_arrays = {}\n",
    "    recall_arrays = {}\n",
    "    for c, name in enumerate(classes_sorted):\n",
    "        p_hist = hist_perclass_precision[:, c].tolist() if ran_epochs else []\n",
    "        r_hist = hist_perclass_recall[:, c].tolist() if ran_epochs else []\n",
    "        precision_arrays[str(name)] = _pad(p_hist, epochs)\n",
    "        recall_arrays[str(name)]    = _pad(r_hist, epochs)\n",
    "\n",
    "    per_class_epoch_arrays = {\"precision\": precision_arrays, \"recall\": recall_arrays}\n",
    "    with open(save_dir / \"per_class_epoch_arrays.json\", \"w\") as f:\n",
    "        json.dump(per_class_epoch_arrays, f, indent=2)\n",
    "\n",
    "    # --- PLOTS ---\n",
    "    # per-class PRECISION\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for c in range(n_classes):\n",
    "        plt.plot(epochs_axis, hist_perclass_precision[:, c], label=str(classes_sorted[c]))\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Precision\"); plt.title(\"Per-class Precision (validation)\")\n",
    "    plt.ylim(0, 1); plt.grid(True, alpha=0.3); plt.legend(fontsize=8, ncol=2)\n",
    "    plt.tight_layout(); plt.savefig(save_dir / \"progress_precision_per_class.png\", dpi=150)\n",
    "\n",
    "    # per-class RECALL (class accuracy)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for c in range(n_classes):\n",
    "        plt.plot(epochs_axis, hist_perclass_recall[:, c], label=str(classes_sorted[c]))\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Class Accuracy (Recall)\"); plt.title(\"Per-class Accuracy (validation)\")\n",
    "    plt.ylim(0, 1); plt.grid(True, alpha=0.3); plt.legend(fontsize=8, ncol=2)\n",
    "    plt.tight_layout(); plt.savefig(save_dir / \"progress_accuracy_per_class.png\", dpi=150)\n",
    "\n",
    "    # overall accuracy + macro-F1\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(epochs_axis, hist_overall_acc, label=\"val accuracy\")\n",
    "    plt.plot(epochs_axis, hist_macro_f1, label=\"val macro F1\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.title(\"Overall Validation Scores\")\n",
    "    plt.ylim(0, 1); plt.grid(True, alpha=0.3); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(save_dir / \"progress_overall.png\", dpi=150)\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # --- Final test evaluation with best model ---\n",
    "    clf = load(save_dir / \"sgd_softmax_best.joblib\")\n",
    "    proba_te = clf.predict_proba(X_te); pred_te = np.argmax(proba_te, axis=1)\n",
    "    acc_te = accuracy_score(y_test_idx, pred_te)\n",
    "    macro_f1_te = f1_score(y_test_idx, pred_te, average=\"macro\")\n",
    "    micro_f1_te = f1_score(y_test_idx, pred_te, average=\"micro\")\n",
    "    top3_te = top_k_accuracy_score(y_test_idx, proba_te, k=min(3, proba_te.shape[1]))\n",
    "    top5_te = top_k_accuracy_score(y_test_idx, proba_te, k=min(5, proba_te.shape[1]))\n",
    "\n",
    "    # per-class (test)\n",
    "    p_test = precision_score(y_test_idx, pred_te, average=None, labels=np.arange(n_classes), zero_division=0.0)\n",
    "    r_test = recall_score(   y_test_idx, pred_te, average=None, labels=np.arange(n_classes), zero_division=0.0)\n",
    "    per_class_test = pd.DataFrame({\n",
    "        \"class_idx\": np.arange(n_classes),\n",
    "        \"class_name\": classes_sorted,\n",
    "        \"precision\": p_test,\n",
    "        \"class_accuracy(recall)\": r_test,\n",
    "        \"support\": np.bincount(y_test_idx, minlength=n_classes)\n",
    "    })\n",
    "    per_class_test.to_csv(save_dir / \"per_class_metrics_test.csv\", index=False)\n",
    "\n",
    "    # save epoch history (flat)\n",
    "    hist_df = pd.DataFrame({\n",
    "        \"epoch\": epochs_axis,\n",
    "        \"overall_val_acc\": hist_overall_acc,\n",
    "        \"val_macro_f1\": hist_macro_f1\n",
    "    })\n",
    "    for c in range(n_classes):\n",
    "        hist_df[f\"prec_{classes_sorted[c]}\"] = hist_perclass_precision[:, c]\n",
    "        hist_df[f\"acc_{classes_sorted[c]}\"]  = hist_perclass_recall[:, c]\n",
    "    hist_df.to_csv(save_dir / \"progress_history.csv\", index=False)\n",
    "\n",
    "    report = {\n",
    "        \"n_classes\": int(n_classes),\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"test_accuracy\": float(acc_te),\n",
    "        \"test_macro_f1\": float(macro_f1_te),\n",
    "        \"test_micro_f1\": float(micro_f1_te),\n",
    "        \"test_top3_accuracy\": float(top3_te),\n",
    "        \"test_top5_accuracy\": float(top5_te),\n",
    "        \"val_frac\": float(val_frac),\n",
    "        \"test_frac\": float(test_frac),\n",
    "    }\n",
    "    with open(save_dir / \"report.json\", \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    print(f\"Printing per class epoch arrays\")\n",
    "    print(per_class_epoch_arrays)\n",
    "\n",
    "    return {\n",
    "        \"report\": report,\n",
    "        \"per_class_epoch_arrays\": per_class_epoch_arrays,  # dict: {precision:{cls:[...]}, recall:{cls:[...]}}\n",
    "        \"paths\": {\n",
    "            \"history_csv\": str((save_dir / \"progress_history.csv\").as_posix()),\n",
    "            \"precision_plot\": str((save_dir / \"progress_precision_per_class.png\").as_posix()),\n",
    "            \"accuracy_plot\": str((save_dir / \"progress_accuracy_per_class.png\").as_posix()),\n",
    "            \"overall_plot\": str((save_dir / \"progress_overall.png\").as_posix()),\n",
    "            \"per_class_test\": str((save_dir / \"per_class_metrics_test.csv\").as_posix()),\n",
    "            \"per_class_epoch_arrays\": str((save_dir / \"per_class_epoch_arrays.json\").as_posix()),\n",
    "            \"clf\": str((save_dir / \"sgd_softmax_best.joblib\").as_posix()),\n",
    "            \"tfidf_word\": str((save_dir / \"tfidf_word.joblib\").as_posix()),\n",
    "            \"tfidf_char\": str((save_dir / \"tfidf_char.joblib\").as_posix()),\n",
    "            \"label_map\": str((save_dir / \"label_map.json\").as_posix()),\n",
    "            \"report\": str((save_dir / \"report.json\").as_posix()),\n",
    "        }\n",
    "    }\n",
    "\n",
    "train_softmax_classifier_classwise(\"./data/training_dataset.csv\",\"./model/training\",ngram_range_char=(1,1),ngram_range_word=(1,1),early_stopping=False,max_features_char=50,max_features_word=50, val_frac=.9, test_frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23cdf4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def mcpt_chance_histogram_auto(\n",
    "    data_path: str,\n",
    "    random_state: int = None,\n",
    "    n_permutations: int = 200,\n",
    "    epochs_mc: int = 2,\n",
    "    batch_size: int = 256,\n",
    "    # vectorizer + clf params mirroring your train function\n",
    "    ngram_range_word=(1, 2),\n",
    "    ngram_range_char=(3, 5),\n",
    "    max_features_word: int = 40000,\n",
    "    max_features_char: int = 50000,\n",
    "    min_df: int = 2,\n",
    "    lr_alpha: float = 1e-5,\n",
    "    learning_rate: str = \"optimal\",\n",
    "    eta0: float = 0.0,\n",
    "    # split fractions (per class) identical to training\n",
    "    val_frac: float = 0.15,\n",
    "    test_frac: float = 0.15,\n",
    "    eval_split: str = \"val\",  # \"val\" or \"test\"\n",
    "    plot: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Monte-Carlo Permutation Test (chance histogram) that:\n",
    "      - loads the same CSV as training,\n",
    "      - uses 'user_input' for X and SECOND COLUMN (6 strings) for y,\n",
    "      - filters classes with >=3 samples,\n",
    "      - does the same per-class split (>=1 per val/test),\n",
    "      - fits TF-IDF on train,\n",
    "      - trains a classifier on *permuted* training labels and evaluates on val/test.\n",
    "    Returns: np.ndarray of accuracies from permutations.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    df = pd.read_csv(Path(data_path))\n",
    "\n",
    "    # X source: text\n",
    "    texts_all = df[\"user_input\"].astype(str).to_numpy()\n",
    "\n",
    "    # y source: SECOND COLUMN (strings)\n",
    "    y_str_all = df.iloc[:, 1].astype(str).to_numpy()\n",
    "\n",
    "    # keep classes with >=3\n",
    "    uniq, counts = np.unique(y_str_all, return_counts=True)\n",
    "    keep = set(uniq[counts >= 3])\n",
    "    mask = np.array([s in keep for s in y_str_all], dtype=bool)\n",
    "    texts = texts_all[mask]\n",
    "    y_str = y_str_all[mask]\n",
    "\n",
    "    # label-encode strings -> 0..C-1\n",
    "    classes_sorted = np.array(sorted(np.unique(y_str)), dtype=object)\n",
    "    str2idx = {s: i for i, s in enumerate(classes_sorted)}\n",
    "    y_idx = np.array([str2idx[s] for s in y_str], dtype=int)\n",
    "\n",
    "    # per-class split with >=1 val/test each\n",
    "    by_class = {c: np.where(y_idx == c)[0] for c in range(len(classes_sorted))}\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for c, idxs in by_class.items():\n",
    "        idxs = idxs.copy()\n",
    "        rng.shuffle(idxs)\n",
    "        n = len(idxs)\n",
    "        n_test = max(1, int(round(test_frac * n)))\n",
    "        n_val  = max(1, int(round(val_frac  * n)))\n",
    "        if n_test + n_val >= n:\n",
    "            if n >= 3:\n",
    "                n_test, n_val = 1, 1\n",
    "            else:\n",
    "                n_test, n_val = 1, 0  # fallback\n",
    "        test_idx.extend(idxs[:n_test])\n",
    "        val_idx.extend(idxs[n_test:n_test + n_val])\n",
    "        train_idx.extend(idxs[n_test + n_val:])\n",
    "\n",
    "    X_train_text = texts[train_idx].tolist()\n",
    "    X_val_text   = texts[val_idx].tolist()\n",
    "    X_test_text  = texts[test_idx].tolist()\n",
    "    y_train_idx  = y_idx[train_idx]\n",
    "    y_val_idx    = y_idx[val_idx]\n",
    "    y_test_idx   = y_idx[test_idx]\n",
    "\n",
    "    # vectorizers (fit on train only)\n",
    "    v_word = TfidfVectorizer(analyzer=\"word\", ngram_range=ngram_range_word,\n",
    "                             min_df=min_df, max_features=max_features_word, lowercase=True)\n",
    "    v_char = TfidfVectorizer(analyzer=\"char\", ngram_range=ngram_range_char,\n",
    "                             min_df=min_df, max_features=max_features_char, lowercase=True)\n",
    "    Xw_tr = v_word.fit_transform(X_train_text); Xc_tr = v_char.fit_transform(X_train_text); X_tr = hstack([Xw_tr, Xc_tr], format=\"csr\")\n",
    "    Xw_va = v_word.transform(X_val_text);       Xc_va = v_char.transform(X_val_text);       X_va = hstack([Xw_va, Xc_va], format=\"csr\")\n",
    "    Xw_te = v_word.transform(X_test_text);      Xc_te = v_char.transform(X_test_text);      X_te = hstack([Xw_te, Xc_te], format=\"csr\")\n",
    "\n",
    "    X_eval = X_va if eval_split == \"val\" else X_te\n",
    "    y_eval = y_val_idx if eval_split == \"val\" else y_test_idx\n",
    "\n",
    "    base_clf = SGDClassifier(loss=\"log_loss\", alpha=lr_alpha,\n",
    "                             learning_rate=learning_rate, eta0=eta0,\n",
    "                             random_state=random_state)\n",
    "\n",
    "    # MC permutations: train on permuted y_train, evaluate on chosen split\n",
    "    n_train = X_tr.shape[0]\n",
    "    accs = np.empty(n_permutations, dtype=float)\n",
    "    for i in range(n_permutations):\n",
    "        y_perm = rng.permutation(y_train_idx)\n",
    "\n",
    "        clf = SGDClassifier(loss=\"log_loss\", alpha=lr_alpha,\n",
    "                            learning_rate=learning_rate, eta0=eta0,\n",
    "                            random_state=rng.integers(0, 2**31-1))\n",
    "\n",
    "        order = np.arange(n_train); rng.shuffle(order)\n",
    "        for ep in range(epochs_mc):\n",
    "            for start in range(0, n_train, batch_size):\n",
    "                end = min(start + batch_size, n_train)\n",
    "                idx = order[start:end]\n",
    "                xb, yb = X_tr[idx], y_perm[idx]\n",
    "                if ep == 0 and start == 0:\n",
    "                    clf.partial_fit(xb, yb, classes=np.arange(len(classes_sorted)))\n",
    "                else:\n",
    "                    clf.partial_fit(xb, yb)\n",
    "\n",
    "        pred = clf.predict(X_eval)\n",
    "        accs[i] = accuracy_score(y_eval, pred)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.hist(accs, bins=30, edgecolor='k')\n",
    "        plt.title(f\"MCPT chance accuracy ({eval_split} split)\")\n",
    "        plt.xlabel(\"Accuracy\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return accs\n",
    "\n",
    "accs = mcpt_chance_histogram_auto(\"./data/training_dataset.csv\",plot=False,n_permutations=2000,ngram_range_char=(1,1),ngram_range_word=(1,1),max_features_char=100,max_features_word=100, val_frac=.9, test_frac=0.01)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(accs, bins=30, edgecolor='k',histtype='stepfilled',color='maroon')\n",
    "plt.vlines(0.96,0,140, colors='maroon', linestyles='--')\n",
    "plt.title(f\"MCPT chance accuracy ({.9} split)\")\n",
    "plt.xlabel(\"Subspecialty Accuracy\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c0aa6",
   "metadata": {},
   "source": [
    "### Condition classifier inference functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_inference as mi\n",
    "\n",
    "mi.inference(user_text=\"\",first_call=True)\n",
    "mi.inference(user_text=\"\", last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)\n",
    "mi.inference(last_ans=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0fd8f",
   "metadata": {},
   "source": [
    "### Script to generate synthetic data (1000 samples default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3333f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(8081)\n",
    "np.random.seed(8081)\n",
    "\n",
    "# ---------- Load source ----------\n",
    "src = Path(\"./symptoms/symptoms_full.csv\")\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "# Normalize headers\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "# Identify columns\n",
    "cond_id_col = next((c for c in df.columns if \"condition\" in c and \"id\" in c), None)\n",
    "cond_name_col = next((c for c in df.columns if c in (\"condition\",\"diagnosis\",\"name\",\"condition_name\")), None)\n",
    "subspec_col = next((c for c in df.columns if \"subspecial\" in c), None)\n",
    "keywords_col = next((c for c in df.columns if \"keyword\" in c), None)\n",
    "\n",
    "if not all([cond_id_col, subspec_col, keywords_col]):\n",
    "    raise RuntimeError(f\"Required columns not found. Got: {df.columns.tolist()}\")\n",
    "\n",
    "# Clean + helpers\n",
    "df = df.dropna(subset=[cond_id_col, subspec_col]).copy()\n",
    "df[cond_id_col] = df[cond_id_col].astype(int)\n",
    "\n",
    "def split_keywords(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).replace(\"|\", \",\").replace(\";\", \",\")\n",
    "    parts = [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "    seen = set(); out = []\n",
    "    for p in parts:\n",
    "        k = p.lower()\n",
    "        if k not in seen:\n",
    "            out.append(p); seen.add(k)\n",
    "    return out\n",
    "\n",
    "df[\"_kw_list\"] = df[keywords_col].apply(split_keywords)\n",
    "df[\"_cond_name\"] = df[cond_name_col] if cond_name_col else \"\"\n",
    "\n",
    "# build neighbor pools by subspecialty\n",
    "by_sub = {}\n",
    "for _, r in df.iterrows():\n",
    "    by_sub.setdefault(str(r[subspec_col]), []).append(r)\n",
    "\n",
    "# ---------- Natural-English rewriting ----------\n",
    "\n",
    "PHRASE_REWRITES = {\n",
    "    \"shortness of breath\": [\"breathlessness\", \"I get winded easily\", \"hard to catch my breath\"],\n",
    "    \"chest pain\": [\"chest tightness\", \"pressure in my chest\", \"pain in my chest\"],\n",
    "    \"pelvic pain\": [\"pelvic discomfort\", \"an ache in my lower abdomen\", \"cramping in my pelvis\"],\n",
    "    \"abdominal pain\": [\"belly pain\", \"stomach ache\", \"abdominal discomfort\"],\n",
    "    \"lower back\": [\"ache in my lower back\", \"low back soreness\", \"low back pain\"],\n",
    "    \"back pain\": [\"back ache\", \"ache in my back\", \"back soreness\"],\n",
    "    \"heavy bleeding\": [\"very heavy periods\", \"heavy flow\", \"soaking pads quickly\"],\n",
    "    \"irregular periods\": [\"my periods are irregular\", \"unpredictable periods\"],\n",
    "    \"painful periods\": [\"painful cramps\", \"cramping with my period\"],\n",
    "    \"vaginal bleeding\": [\"vaginal spotting\", \"unexpected vaginal bleeding\"],\n",
    "    \"urinary frequency\": [\"peeing often\", \"going to the bathroom a lot\"],\n",
    "    \"urinary urgency\": [\"a sudden urge to pee\", \"needing to pee right away\"],\n",
    "    \"burning urination\": [\"it burns when I pee\", \"pain when urinating\"],\n",
    "    \"dyspareunia\": [\"pain with sex\", \"sex is painful\"],\n",
    "    \"nausea\": [\"feeling queasy\", \"nauseated\"],\n",
    "    \"vomiting\": [\"throwing up\", \"vomiting\"],\n",
    "    \"diarrhea\": [\"loose stools\", \"the runs\"],\n",
    "    \"constipation\": [\"hard to pass stool\", \"constipation\"],\n",
    "    \"bloating\": [\"feeling bloated\", \"abdominal swelling\"],\n",
    "    \"fatigue\": [\"low energy\", \"feeling exhausted\", \"tired all the time\"],\n",
    "    \"dizziness\": [\"feeling lightheaded\", \"woozy spells\"],\n",
    "    \"headache\": [\"head pain\", \"throbbing headaches\", \"migraine-like pain\"],\n",
    "    \"itching\": [\"itchiness\", \"persistent itching\"],\n",
    "    \"discharge\": [\"unusual discharge\", \"unusual fluid\"],\n",
    "    \"incontinence\": [\"leaking urine\", \"urine leakage\"],\n",
    "    \"hematuria\": [\"blood in my urine\", \"pink or red urine\"],\n",
    "    \"overactive bladder\": [\"urgent and frequent urination\", \"OAB-like symptoms\"],\n",
    "    \"urinary incontinence\": [\"leaking urine\", \"stress incontinence\"],\n",
    "}\n",
    "\n",
    "TOKEN_SWAPS = {\n",
    "    \"pain\": [\"ache\", \"discomfort\", \"soreness\"],\n",
    "    \"bleeding\": [\"spotting\", \"blood loss\"],\n",
    "    \"cramps\": [\"cramping\", \"spasms\"],\n",
    "    \"swelling\": [\"bloating\", \"puffiness\"],\n",
    "    \"burning\": [\"stinging\", \"burning sensation\"],\n",
    "    \"discharge\": [\"fluid\", \"secretions\"],\n",
    "    \"urgency\": [\"urgent need\", \"urge\"],\n",
    "    \"frequency\": [\"frequent\", \"often\"],\n",
    "}\n",
    "\n",
    "DURATIONS = [\n",
    "    \"for a few days\", \"for two weeks\", \"for several months\", \"on and off\",\n",
    "    \"since yesterday\", \"recently\", \"over the past month\"\n",
    "]\n",
    "MODIFIERS = [\n",
    "    \"worse at night\", \"after exercise\", \"during my period\", \"after intercourse\",\n",
    "    \"with a low-grade fever\", \"with nausea\", \"with low back ache\",\n",
    "    \"especially when standing\", \"when I cough or sneeze\", \"worse in the morning\"\n",
    "]\n",
    "\n",
    "TEMPLATES = [\n",
    "    \"I've been dealing with {kws} {duration}, {mod}.\",\n",
    "    \"Lately I've noticed {kws} {duration}.\",\n",
    "    \"{kws} {duration}. Should I be concerned?\",\n",
    "    \"For {duration}, it's been {kws}. Any guidance?\",\n",
    "    \"I keep getting {kws} {duration}, {mod}.\",\n",
    "    \"Having {kws} {duration}. What could this be?\",\n",
    "    \"Symptoms include {kws} {duration}.\",\n",
    "    \"It's mostly {kws} {duration}, {mod}.\",\n",
    "    \"Recently it's been {kws}.\",\n",
    "    \"Persistent {kws} {duration}.\",\n",
    "]\n",
    "\n",
    "ANAT_LOCS = [\n",
    "    \"lower back\", \"pelvis\", \"pelvic area\", \"abdomen\", \"stomach\", \"chest\",\n",
    "    \"throat\", \"groin\", \"hip\", \"knee\", \"shoulder\", \"neck\", \"flank\", \"side\"\n",
    "]\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n",
    "\n",
    "def rewrite_phrase(phrase: str) -> str:\n",
    "    base = phrase.strip()\n",
    "    low = _norm(base)\n",
    "    # phrase-level\n",
    "    for k, opts in PHRASE_REWRITES.items():\n",
    "        if k in low:\n",
    "            return random.choice(opts)\n",
    "    # token-level\n",
    "    toks = base.split()\n",
    "    changed = False\n",
    "    for i, tok in enumerate(toks):\n",
    "        core = re.sub(r\"[^\\w-]\", \"\", tok.lower())\n",
    "        if core in TOKEN_SWAPS and random.random() < 0.6:\n",
    "            toks[i] = random.choice(TOKEN_SWAPS[core])\n",
    "            changed = True\n",
    "    if changed:\n",
    "        return \" \".join(toks)\n",
    "    # grammar tweak: \"<loc> pain\" -> \"pain in my <loc>\"\n",
    "    if \"pain\" in low:\n",
    "        for loc in ANAT_LOCS:\n",
    "            if loc in low:\n",
    "                return random.choice([f\"pain in my {loc}\", f\"an ache in my {loc}\", f\"{loc} pain\"])\n",
    "        return random.choice([\"ongoing pain\", \"persistent pain\", base])\n",
    "    return base\n",
    "\n",
    "def list_join(items):\n",
    "    if not items: return \"\"\n",
    "    if len(items) == 1: return items[0]\n",
    "    return \", \".join(items[:-1]) + f\", and {items[-1]}\"\n",
    "\n",
    "def choose_keywords(base_list, neighbor_pool, kmin=3, kmax=6):\n",
    "    # base_list: original row keywords\n",
    "    # neighbor_pool: list of additional keywords from neighbors\n",
    "    pool = list(dict.fromkeys(base_list + neighbor_pool))  # de-dup, keep order\n",
    "    if not pool:\n",
    "        pool = [\"fatigue\", \"bloating\"]\n",
    "    k = min(len(pool), random.randint(kmin, kmax))\n",
    "    if k == 0: k = 1\n",
    "    random.shuffle(pool)\n",
    "    return pool[:k]\n",
    "\n",
    "# Build a mapping subspecialty -> pool of other keywords\n",
    "sub_kw_pool = {}\n",
    "for sub, rows in by_sub.items():\n",
    "    kw_accum = []\n",
    "    for rr in rows:\n",
    "        kw_accum.extend(rr[\"_kw_list\"] or [])\n",
    "    # de-duplicate, keep insertion order\n",
    "    seen = set(); pooled = []\n",
    "    for w in kw_accum:\n",
    "        lw = w.lower()\n",
    "        if lw not in seen:\n",
    "            pooled.append(w); seen.add(lw)\n",
    "    sub_kw_pool[sub] = pooled\n",
    "\n",
    "# ---------- Create variants: 10 per row ----------\n",
    "variants_per_row = 10\n",
    "rows_out = []\n",
    "\n",
    "for idx, r in df.iterrows():\n",
    "    base_id = int(r[cond_id_col])\n",
    "    subspec = str(r[subspec_col])\n",
    "    base_kws = r[\"_kw_list\"] or []\n",
    "    neighbor_pool = sub_kw_pool.get(subspec, [])\n",
    "    # remove base keywords from neighbor pool to emphasize \"other sets\"\n",
    "    neighbor_pool_filtered = [w for w in neighbor_pool if _norm(w) not in {_norm(x) for x in base_kws}]\n",
    "\n",
    "    for v in range(1, variants_per_row + 1):\n",
    "        # pick a set of keywords (some from base, some from neighbor pool)\n",
    "        chosen = choose_keywords(\n",
    "            base_list=base_kws,\n",
    "            neighbor_pool=random.sample(neighbor_pool_filtered, k=min(len(neighbor_pool_filtered), random.randint(2,6))),\n",
    "            kmin=3, kmax=6\n",
    "        )\n",
    "        # rewrite some to natural English; ensure at least one exact from original base_kws remains\n",
    "        realized = []\n",
    "        had_exact_from_base = False\n",
    "        base_set_norm = {_norm(x) for x in base_kws}\n",
    "        for j, kw in enumerate(chosen):\n",
    "            if _norm(kw) in base_set_norm and random.random() < (0.55 if j == 0 else 0.4):\n",
    "                realized.append(kw)\n",
    "                had_exact_from_base = True\n",
    "            else:\n",
    "                realized.append(rewrite_phrase(kw))\n",
    "        if not had_exact_from_base and base_kws:\n",
    "            realized[0] = base_kws[0]\n",
    "\n",
    "        # build user_input\n",
    "        sentence = random.choice(TEMPLATES).format(\n",
    "            kws=list_join(realized),\n",
    "            duration=random.choice(DURATIONS),\n",
    "            mod=random.choice(MODIFIERS)\n",
    "        )\n",
    "        rows_out.append({\n",
    "            \"source_condition_id\": base_id,\n",
    "            \"source_subspecialty\": subspec,\n",
    "            \"variant_index\": v,\n",
    "            \"variant_keywords\": json.dumps(chosen),\n",
    "            \"user_input\": sentence\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "out_path = Path(\"./symptoms/keyword_sets_user_inputs.csv\")\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"source_condition_id\",\"source_subspecialty\",\"variant_index\",\"variant_keywords\",\"user_input\"], quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows_out)\n",
    "\n",
    "# Preview a slice\n",
    "preview = pd.DataFrame(rows_out).head(25)\n",
    "\n",
    "out_path.as_posix()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66511a96",
   "metadata": {},
   "source": [
    "## MOVING FORWARD IN MORNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd09bc",
   "metadata": {},
   "source": [
    "TO DO FOR LOGAN OR FOR BACKEND:\n",
    "- Create subspecialty to subspecialty_key csv file to force static index all the time.\n",
    "- Instantiate algorithm for subspecialty selection (Vector of percent): where $c_i$ is some predicted condition from user input\n",
    "$$\\text{subspecialty\\_chance}_j = \\sum_{i=0}^{k}{c_i1\\{b(i)=j\\}}, \\forall j \\in [1,s]$$\n",
    "- Instantiate algorithm for doctor selection (vector of percent): where $\\vec{d}_i$ is the experience level of each doctor on some given condition $i$\n",
    " $$\\text{doctor\\_chance}_j = ln(\\sum_{i=0}^{k}{d_j^{(c_i)}})$$\n",
    "- Consider power sharpening these\n",
    "- make SINGLE function that takes user input string and converts into subspecialty pred vector and doctor pred vector\n",
    "#### Stretch goals\n",
    "- MCPT on data\n",
    "- various visualizations of model rate of learning\n",
    "- all for the construction of \"more info\" page\n",
    "- Build lifting stuff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('./data/symptoms_full.csv')\n",
    "sspec_data = pd.read_csv('./data/sspec_key_map.csv')\n",
    "\n",
    "sympt = data.values\n",
    "\n",
    "sspec = sspec_data.values\n",
    "\n",
    "for i in range(6):\n",
    "    \n",
    "    loc = np.where(sympt[:, 1]==sspec[i,2])\n",
    "    \n",
    "    sympt[loc, 1] = sspec[i, 0]\n",
    "\n",
    "print(sympt)\n",
    "\n",
    "new_df = pd.DataFrame(data=sympt, columns=data.columns)\n",
    "new_df = new_df.drop(columns=['subspecialty'])\n",
    "new_df.to_csv('./data/symptoms_full_i.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< Updated upstream
   "display_name": ".venv",
=======
   "display_name": ".venv (3.12.3)",
>>>>>>> Stashed changes
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.12.11"
=======
   "version": "3.12.3"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
